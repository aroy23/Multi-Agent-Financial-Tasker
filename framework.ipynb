{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain_community langchain_experimental langgraph langchain langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "def _set_if_undefined(var : str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n",
    "\n",
    "_set_if_undefined(\"OPENAI_API_KEY\")\n",
    "_set_if_undefined(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_system_prompt(suffix: str) -> str:\n",
    "    return (\n",
    "        \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "        \" Use the provided tools to progress towards answering the question.\"\n",
    "        \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "        \" will help where you left off. Execute what you can to make progress.\"\n",
    "        \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "        \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "        f\"\\n{suffix}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.graph import MessagesState, END\n",
    "from langgraph.types import Command\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "def get_next_node(last_message: BaseMessage, goto: str):\n",
    "    if \"FINAL ANSWER\" in last_message.content:\n",
    "        return END\n",
    "    return goto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_agent = create_react_agent(\n",
    "    llm,\n",
    "    tools=[tavily_tool],\n",
    "    prompt=make_system_prompt(\n",
    "        \"You can only perform research on the latest financial market data from the current year. \"\n",
    "        \"Work with your colleague to generate investment advice.\"\n",
    "        \"You should not generate investment advice, leave that to your colleague.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_node(state: MessagesState) -> Command[Literal[\"investment_advisor\", END]]:\n",
    "    result = research_agent.invoke(state)\n",
    "    goto = get_next_node(result[\"messages\"][-1], \"investment_advisor\")\n",
    "    result[\"messages\"][-1] = HumanMessage(content=result[\"messages\"][-1].content, name=\"researcher\")\n",
    "    return Command(\n",
    "        update={\"messages\": result[\"messages\"]},\n",
    "        goto=goto,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investment_agent = create_react_agent(\n",
    "    llm,\n",
    "    tools=[],\n",
    "    prompt=make_system_prompt(\n",
    "        \"You can only generate investment predictions and market advice based on the research data provided. \"\n",
    "        \"Support your advice with quantitative metrics like stock price percent changes, key ratios, and other relevant data.\"\n",
    "        \"Collaborate with your researcher colleague to produce the final answer.\"\n",
    "        \"If the question is regarding investment the final answer should point to one direction: invest or do not invest.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investment_node(state: MessagesState) -> Command[Literal[\"researcher\", END]]:\n",
    "    result = investment_agent.invoke(state)\n",
    "    goto = get_next_node(result[\"messages\"][-1], \"researcher\")\n",
    "    result[\"messages\"][-1] = HumanMessage(content=result[\"messages\"][-1].content, name=\"investment_advisor\")\n",
    "    return Command(\n",
    "        update={\"messages\": result[\"messages\"]},\n",
    "        goto=goto,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"researcher\", research_node)\n",
    "workflow.add_node(\"investment_advisor\", investment_node)\n",
    "workflow.add_edge(START, \"researcher\")\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\"user\", \"How is the S&P 500 growth looking like for 2025?\")\n",
    "        ],\n",
    "    },\n",
    "    {\"recursion_limit\": 150},\n",
    ")\n",
    "data = None\n",
    "for event in events:     \n",
    "    data = event.get(\"investment_advisor\") or event.get(\"researcher\")\n",
    "if data:\n",
    "    print(data[\"messages\"][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
